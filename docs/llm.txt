# Agent Orcha - LLM Documentation

## Project Overview

Agent Orcha is a declarative TypeScript framework for building, managing, and scaling multi-agent AI systems. It combines the flexibility of TypeScript with the simplicity of YAML to orchestrate complex workflows, manage diverse tools via Model Context Protocol (MCP), and integrate semantic search and knowledge graphs seamlessly.

**Repository:** https://github.com/ddalcu/agent-orcha
**NPM Package:** https://www.npmjs.com/package/agent-orcha
**Version:** 0.0.3
**License:** MIT
**Requirements:** Node.js >= 20.0.0

## Key Features

1. **Declarative AI**: Define agents, workflows, and infrastructure in version-controlled YAML files
2. **Model Agnostic**: Swap between OpenAI, Gemini, Anthropic, or local LLMs (Ollama, LM Studio) without code changes
3. **Universal Tooling**: Leverage Model Context Protocol (MCP) to connect agents to any external service, API, or database
4. **Knowledge Stores**: Built-in vector stores (Chroma, Pinecone, Qdrant, Memory) and GraphRAG knowledge graphs for semantic search, entity analysis, and knowledge retrieval
5. **LangGraph Workflows**: Autonomous prompt-driven workflows with ReAct/single-turn modes, tool/agent discovery, and human-in-the-loop
6. **Robust Workflow Engine**: Orchestrate complex multi-agent sequences with parallel execution, conditional logic, and state management
7. **Agent Orcha Studio**: Built-in web dashboard with agent testing, knowledge browsing, workflow execution, MCP management, and in-browser IDE
8. **Conversation Memory**: Session-based memory with FIFO management and automatic TTL cleanup
9. **Structured Output**: JSON schema validation for agent responses
10. **Extensible Functions**: Custom JavaScript functions to extend agent capabilities with zero boilerplate
11. **AI-First Design**: YAML configurations are ideal for LLMs to read, write, and maintain

## Quick Start

### 1. Initialize Project

```bash
npx agent-orcha init my-project
cd my-project
```

This creates a project with:
- agents/ - Agent configuration files
- workflows/ - Workflow definition files
- functions/ - Custom JavaScript functions
- knowledge/ - Knowledge store configurations and data
- llm.json - LLM and embedding provider settings
- mcp.json - MCP server configuration

### 2. Configure LLM (llm.json)

```json
{
  "version": "1.0",
  "models": {
    "default": {
      "provider": "openai",
      "baseUrl": "http://localhost:1234/v1",
      "apiKey": "not-needed",
      "model": "your-model-name",
      "temperature": 0.7
    },
    "openai": {
      "apiKey": "sk-your-openai-key",
      "model": "gpt-4o",
      "temperature": 0.7
    }
  },
  "embeddings": {
    "default": {
      "provider": "openai",
      "baseUrl": "http://localhost:1234/v1",
      "apiKey": "not-needed",
      "model": "text-embedding-model"
    }
  }
}
```

**LLM Providers:**
- OpenAI: Omit baseUrl (uses default OpenAI endpoint)
- LM Studio: baseUrl: "http://localhost:1234/v1"
- Ollama: baseUrl: "http://localhost:11434/v1"
- Anthropic: Supported via provider configuration
- Gemini: Supported via provider configuration

### 3. Create Agent (agents/myagent.agent.yaml)

```yaml
name: myagent
description: My first AI agent
version: "1.0.0"

llm:
  name: default
  temperature: 0.7

prompt:
  system: |
    You are a helpful assistant.
    Answer questions clearly and concisely.
  inputVariables:
    - query

output:
  format: text
```

### 4. Start Server

```bash
npx agent-orcha start
# Server runs on http://localhost:3000
# Agent Orcha Studio available at http://localhost:3000
```

### 5. Invoke Agent

```bash
curl -X POST http://localhost:3000/api/agents/myagent/invoke \
  -H "Content-Type: application/json" \
  -d '{"input": {"query": "Hello, how are you?"}}'
```

Response:
```json
{
  "output": "Hello! I'm doing well, thank you...",
  "metadata": {
    "tokensUsed": 42,
    "toolCalls": [],
    "duration": 823
  }
}
```

## Core Concepts

### Agents

Agents are AI-powered units that use LLMs to process input and optionally call tools. Each agent is defined in a YAML file.

**Agent Schema:**
```yaml
name: string                    # Unique identifier (required)
description: string             # Human-readable description (required)
version: string                 # Semantic version (default: "1.0.0")

llm: string | object            # LLM configuration reference
  # Simple: llm: default
  # With override: llm: { name: default, temperature: 0.3 }

prompt:                         # Prompt configuration (required)
  system: string                # System message/instructions
  inputVariables: [string]      # Variables to interpolate in prompt

tools:                          # Available tools (optional)
  - mcp:<server-name>           # MCP server tools
  - knowledge:<store-name>      # Knowledge store search
  - function:<function-name>    # Custom functions
  - builtin:<tool-name>         # Built-in tools (e.g., ask_user)

output:                         # Output formatting (optional)
  format: text | json | structured
  schema:                       # Required when format is "structured"
    type: object
    properties:
      <field>:
        type: string | number | boolean | array | object
        description: string
        enum: [values]
    required: [field1, field2]

metadata:                       # Custom metadata (optional)
  category: string
  tags: [string]
```

**Example Agent with Tools:**
```yaml
name: researcher
description: Researches topics using web fetch and knowledge search
version: "1.0.0"

llm:
  name: default
  temperature: 0.5

prompt:
  system: |
    You are a thorough researcher.
    Use available tools to gather information before responding.
  inputVariables:
    - topic
    - context

tools:
  - mcp:fetch              # Web fetch MCP server
  - knowledge:docs         # Knowledge store search
  - function:custom-tool   # Custom function

output:
  format: text
```

**Agent with Structured Output:**
```yaml
name: sentiment-analyzer
description: Analyzes sentiment with structured output
version: "1.0.0"

llm:
  name: default
  temperature: 0

prompt:
  system: |
    Analyze the sentiment of the provided text.
  inputVariables:
    - text

output:
  format: structured
  schema:
    type: object
    properties:
      sentiment:
        type: string
        enum: [positive, negative, neutral]
      confidence:
        type: number
        minimum: 0
        maximum: 1
      keywords:
        type: array
        items:
          type: string
    required:
      - sentiment
      - confidence
```

### Conversation Memory

Agents automatically support conversation memory when a `sessionId` is provided in API calls. No agent-level configuration is required.

```bash
# First message in a session
curl -X POST http://localhost:3000/api/agents/chatbot/invoke \
  -H "Content-Type: application/json" \
  -d '{"input": {"message": "My name is Alice"}, "sessionId": "user-123"}'

# Follow-up message (agent remembers context)
curl -X POST http://localhost:3000/api/agents/chatbot/invoke \
  -H "Content-Type: application/json" \
  -d '{"input": {"message": "What is my name?"}, "sessionId": "user-123"}'
```

Memory settings (global, configured in orchestrator):
- `maxMessagesPerSession`: 50 (default) - FIFO eviction when exceeded
- `sessionTTL`: optional, in milliseconds - automatic cleanup of expired sessions

Session management endpoints:
- `GET /api/agents/sessions/stats` - Session statistics
- `GET /api/agents/sessions/:sessionId` - Get session details
- `DELETE /api/agents/sessions/:sessionId` - Clear session messages

### Workflows

Workflows orchestrate multiple agents. Two types are supported: step-based (`type: steps`) and LangGraph (`type: langgraph`).

#### Step-Based Workflows

Step-based workflows define explicit sequences with support for parallel execution, conditional logic, and state management.

**Workflow Schema:**
```yaml
name: string                    # Unique identifier (required)
description: string             # Human-readable description (required)
version: string                 # Semantic version (default: "1.0.0")
type: steps                     # Optional, "steps" is default

input:                          # Input schema (required)
  schema:
    <field_name>:
      type: string | number | boolean | array | object
      required: boolean
      default: any
      description: string

steps:                          # Workflow steps (required)
  - id: string                  # Unique step identifier
    agent: string               # Agent name to execute
    input:                      # Input mapping using templates
      <key>: "{{input.field}}"           # From workflow input
      <key>: "{{steps.stepId.output}}"   # From previous step
    condition: string           # Optional conditional execution
    retry:                      # Optional retry configuration
      maxAttempts: number       # Default: 3
      delay: number             # Milliseconds (default: 1000)
    output:
      key: string

  # Parallel execution
  - parallel:
      - id: step1
        agent: agent1
        input: {...}
      - id: step2
        agent: agent2
        input: {...}

config:                         # Workflow configuration (optional)
  timeout: number               # Total timeout ms (default: 300000)
  onError: stop | continue | retry

output:                         # Output mapping (required)
  <key>: "{{steps.stepId.output}}"

metadata:                       # Custom metadata (optional)
  category: string
  tags: [string]
```

**Template Syntax:**
- `{{input.fieldName}}` - Access workflow input field
- `{{steps.stepId.output}}` - Access step output
- `{{steps.stepId.output.nested.path}}` - Access nested output
- `{{steps.stepId.metadata.duration}}` - Access step metadata

**Example Step-Based Workflow:**
```yaml
name: research-paper
description: Research a topic and write a comprehensive paper
version: "1.0.0"

input:
  schema:
    topic:
      type: string
      required: true
      description: The topic to research
    style:
      type: string
      default: "professional"

steps:
  - id: research
    agent: researcher
    input:
      topic: "{{input.topic}}"
      context: "Gather comprehensive information"
    output:
      key: researchFindings

  - id: summarize
    agent: summarizer
    input:
      content: "{{steps.research.output}}"
      maxPoints: "10"
    condition: "{{steps.research.metadata.success}}"
    output:
      key: summary

  - id: write
    agent: writer
    input:
      research: "{{steps.research.output}}"
      outline: "{{steps.summarize.output}}"
      style: "{{input.style}}"
    output:
      key: paper

config:
  timeout: 600000
  onError: stop

output:
  paper: "{{steps.write.output}}"
  summary: "{{steps.summarize.output}}"
  researchFindings: "{{steps.research.output}}"
```

#### LangGraph Workflows

Autonomous, prompt-driven workflows using LangGraph. The agent decides which tools and agents to call based on the prompt.

**LangGraph Schema:**
```yaml
name: string                    # Unique identifier (required)
description: string             # Human-readable description (required)
version: string                 # Semantic version (default: "1.0.0")
type: langgraph                 # Required for LangGraph workflows

input:                          # Input schema (required)
  schema:
    <field_name>:
      type: string | number | boolean | array | object
      required: boolean
      description: string

prompt:                         # Prompt configuration (required)
  system: string                # System message with instructions
  goal: string                  # Goal template (supports {{input.*}} interpolation)

graph:                          # LangGraph configuration (required)
  model: string                 # LLM config name from llm.json

  executionMode: react | single-turn  # Default: react
  # react: Full ReAct loop, multiple rounds of tool calls
  # single-turn: Calls tools once and returns

  tools:                        # Tool discovery config
    mode: all | include | exclude | none  # Default: all
    sources:                    # Tool source types (default: all)
      - mcp
      - knowledge
      - function
      - builtin
    include: [string]           # Tool names to include (for mode: include)
    exclude: [string]           # Tool names to exclude (for mode: exclude)

  agents:                       # Agent discovery config
    mode: all | include | exclude | none  # Default: all
    include: [string]           # Agent names to include
    exclude: [string]           # Agent names to exclude

  maxIterations: number         # Max tool-calling iterations (default: 10)
  timeout: number               # Timeout in ms (default: 300000)

output:                         # Output extraction (required)
  <key>: "{{state.messages[-1].content}}"

config:                         # Workflow configuration (optional)
  onError: stop | continue | retry

metadata:                       # Custom metadata (optional)
  category: string
  tags: [string]
```

**Execution Modes:**

| Mode | Behavior | Best For |
|------|----------|----------|
| `single-turn` | Calls tools once, then returns | Research, data gathering, straightforward tasks |
| `react` | Multiple rounds of tool calls with reasoning | Complex multi-step problems, iterative analysis |

**Human-in-the-Loop:** LangGraph workflows support the `builtin:ask_user` tool. When called, the workflow pauses (via `NodeInterrupt`) and waits for user input. Include `builtin` in tool sources to enable it.

**Example LangGraph Workflow:**
```yaml
name: langgraph-research
description: Autonomous research using tool discovery
version: "1.0.0"
type: langgraph

input:
  schema:
    topic:
      type: string
      required: true

prompt:
  system: |
    You are a research assistant with access to tools and agents.
    Identify all tools you need, call them in parallel, then synthesize results.
  goal: "Research and analyze: {{input.topic}}"

graph:
  model: default
  executionMode: single-turn
  tools:
    mode: all
    sources: [mcp, knowledge, function, builtin]
  agents:
    mode: all
  maxIterations: 10
  timeout: 300000

output:
  analysis: "{{state.messages[-1].content}}"
```

### Knowledge Stores

Knowledge stores enable semantic search and RAG capabilities. They are configured in `*.knowledge.yaml` files in the `knowledge/` directory. Two kinds: `vector` (default) and `graph-rag`.

#### Vector Knowledge Stores

Traditional vector stores with embeddings for semantic search.

**Vector Store Schema:**
```yaml
name: string                    # Unique identifier (required)
description: string             # Human-readable description (required)
kind: vector                    # Optional (vector is default)

source:                         # Data source (required)
  # Directory source
  type: directory
  path: string                  # Path relative to project root
  pattern: string               # Glob pattern for directories
  recursive: boolean            # Recursive search (default: true)

  # File source
  type: file
  path: string                  # Single file path

  # Database source
  type: database
  connectionString: string      # postgresql:// or mysql://
  query: string                 # SQL query
  contentColumn: string         # Column with content (default: "content")
  metadataColumns: [string]     # Columns for metadata
  batchSize: number             # Rows per batch (default: 100)

  # Web source
  type: web
  url: string                   # URL to scrape
  selector: string              # CSS selector (optional)
  headers:                      # Custom headers (optional)
    Authorization: "Bearer TOKEN"

  # S3 source
  type: s3
  bucket: string                # S3 bucket name
  prefix: string                # Folder filter (optional)
  endpoint: string              # Custom S3 endpoint (for MinIO/Wasabi)
  region: string                # Default: us-east-1
  accessKeyId: string           # Optional (uses env vars if omitted)
  secretAccessKey: string       # Optional (uses env vars if omitted)
  pattern: string               # File glob pattern (optional)
  forcePathStyle: boolean       # Default: false (set true for MinIO)

loader:                         # Document loader (required)
  type: text | pdf | csv | json | markdown

splitter:                       # Text chunking (required)
  type: character | recursive | token | markdown
  chunkSize: number             # Characters per chunk (default: 1000)
  chunkOverlap: number          # Overlap between chunks (default: 200)

embedding: string               # Reference to embedding config (default: "default")

store:                          # Vector store backend (required)
  type: memory | chroma | pinecone | qdrant
  options:
    path: string                # Storage path (for chroma)
    collectionName: string      # Collection name
    url: string                 # Server URL (for chroma, qdrant)

search:                         # Search configuration (optional)
  defaultK: number              # Results per search (default: 4)
  scoreThreshold: number        # Minimum similarity (0-1)
```

**Store Types:**
1. **Memory** - In-memory storage, fast but not persistent, recreates embeddings on startup
2. **Chroma** - Persistent local storage, embeddings cached and reused across restarts
3. **Pinecone** - Managed cloud vector database
4. **Qdrant** - High-performance open-source vector database

**Example Vector Store:**
```yaml
name: docs
description: Knowledge base for semantic search

source:
  type: directory
  path: knowledge/sample-data
  pattern: "*.txt"

loader:
  type: text

splitter:
  type: character
  chunkSize: 1000
  chunkOverlap: 200

embedding: default

store:
  type: memory

search:
  defaultK: 4
  scoreThreshold: 0.2
```

#### GraphRAG Knowledge Stores

Entity extraction and knowledge graph with community detection for advanced analysis.

**How GraphRAG Works:**
1. **Extraction**: Documents are split and an LLM extracts entities and relationships
2. **Graph Building**: Entities become nodes, relationships become edges
3. **Community Detection**: Louvain algorithm groups related entities into communities
4. **Community Summaries**: An LLM generates summaries for each community
5. **Search**: Local search finds specific entities; global search analyzes community summaries

**GraphRAG Schema:**
```yaml
name: string                    # Unique identifier (required)
kind: graph-rag                 # Required for GraphRAG
description: string             # Human-readable description (required)

source:                         # Same source types as vector (required)
  type: directory | file | database | web | s3

loader:                         # Document loader (required)
  type: text | pdf | csv | json | markdown

splitter:                       # Text chunking (required)
  type: character | recursive | token | markdown
  chunkSize: number
  chunkOverlap: number

embedding: string               # Embedding config name from llm.json

graph:                          # Graph configuration (required)
  extraction:                   # Entity extraction config
    llm: string                 # LLM name from llm.json (default: "default")
    entityTypes:                # Optional - omit for automatic extraction
      - name: string
        description: string
    relationshipTypes:          # Optional - omit for automatic extraction
      - name: string
        description: string

  communities:                  # Community detection config
    algorithm: louvain          # Only supported algorithm
    resolution: number          # Louvain resolution (default: 1.0)
    minSize: number             # Min community size (default: 2)
    summaryLlm: string          # LLM for community summaries (default: "default")

  store:                        # Graph store backend
    type: memory | neo4j        # Default: memory
    options: {}                 # Store-specific options

  cache:                        # Graph cache config
    enabled: boolean            # Default: true
    directory: string           # Default: ".graph-cache"

search:                         # Search configuration (optional)
  defaultK: number              # Results per search (default: 10)
  localSearch:                  # Entity neighborhood search
    maxDepth: number            # Graph traversal depth (default: 2)
  globalSearch:                 # Community-level search
    topCommunities: number      # Communities to consider (default: 5)
    llm: string                 # LLM for synthesis (default: "default")
```

**Example GraphRAG Store:**
```yaml
name: call-center-analysis
kind: graph-rag
description: GraphRAG for analyzing call center transcripts

source:
  type: directory
  path: knowledge/transcripts
  pattern: "*.txt"
  recursive: true

loader:
  type: text

splitter:
  type: recursive
  chunkSize: 2000
  chunkOverlap: 200

embedding: default

graph:
  extraction:
    llm: default
    entityTypes:
      - name: Agent
        description: "Call center representative"
      - name: Customer
        description: "Person calling"
      - name: Vehicle
        description: "Car discussed"
      - name: Outcome
        description: "Result of the call"
    relationshipTypes:
      - name: HANDLED_BY
        description: "Call was handled by an agent"
      - name: INTERESTED_IN
        description: "Customer interest in vehicle"
      - name: RESULTED_IN
        description: "Call resulted in outcome"

  communities:
    algorithm: louvain
    resolution: 1.0
    minSize: 2
    summaryLlm: default

  store:
    type: memory

  cache:
    enabled: true
    directory: .graph-cache

search:
  defaultK: 10
  localSearch:
    maxDepth: 2
  globalSearch:
    topCommunities: 5
    llm: default
```

### Functions

Functions are custom JavaScript tools that extend agent capabilities. They require no dependencies and are simple to create.

**Function Schema:**
```javascript
export default {
  name: 'function-name',           // Unique identifier (required)
  description: 'What it does',     // Clear description (required)

  parameters: {                    // Input parameters (required)
    param1: {
      type: 'number',              // string | number | boolean | array | object | enum
      description: 'Parameter description',
      required: true,              // Optional, defaults to true
      default: 0,                  // Optional default value
    },
  },

  execute: async ({ param1 }) => { // Execution function (required)
    // Your logic here
    return `Result: ${param1}`;
  },
};

// Optional metadata
export const metadata = {
  name: 'function-name',
  version: '1.0.0',
  author: 'Your Name',
  tags: ['category'],
};
```

**Example Function (Calculator):**
```javascript
export default {
  name: 'calculator',
  description: 'Performs basic arithmetic operations',

  parameters: {
    a: { type: 'number', description: 'First number' },
    b: { type: 'number', description: 'Second number' },
    operation: {
      type: 'enum',
      values: ['add', 'subtract', 'multiply', 'divide'],
      description: 'Operation to perform',
    },
  },

  execute: async ({ a, b, operation }) => {
    switch (operation) {
      case 'add': return `${a} + ${b} = ${a + b}`;
      case 'subtract': return `${a} - ${b} = ${a - b}`;
      case 'multiply': return `${a} * ${b} = ${a * b}`;
      case 'divide': return `${a} / ${b} = ${a / b}`;
    }
  },
};
```

**Using Functions in Agents:**
```yaml
name: math-assistant
description: Assistant that can perform calculations

llm:
  name: default
  temperature: 0.3

prompt:
  system: |
    You are a math assistant.
    Use the calculator tool to perform calculations.
  inputVariables:
    - query

tools:
  - function:calculator    # References calculator.function.js

output:
  format: text
```

### MCP Servers

Model Context Protocol (MCP) servers provide external tools to agents. Configure them in mcp.json.

**MCP Configuration:**
```json
{
  "version": "1.0.0",
  "servers": {
    "<server-name>": {
      "transport": "streamable-http | stdio | sse | sse-only",
      "url": "https://server-url/mcp",
      "headers": { "key": "value" },
      "command": "node",
      "args": ["./mcp-server.js"],
      "env": { "KEY": "VALUE" },
      "timeout": 30000,
      "enabled": true,
      "description": "Server description"
    }
  },
  "globalOptions": {
    "throwOnLoadError": false,
    "prefixToolNameWithServerName": true,
    "additionalToolNamePrefix": "",
    "defaultToolTimeout": 30000
  }
}
```

**Transport Types:**

| Transport | Use Case | Required Fields |
|-----------|----------|----------------|
| `stdio` | Local CLI tools | `command`, `args` |
| `streamable-http` | Remote HTTP servers | `url` |
| `sse` | Server-Sent Events | `url` |
| `sse-only` | SSE without HTTP fallback | `url` |

**Example MCP Configuration:**
```json
{
  "version": "1.0.0",
  "servers": {
    "fetch": {
      "transport": "streamable-http",
      "url": "https://remote.mcpservers.org/fetch/mcp",
      "description": "Web fetch capabilities",
      "timeout": 30000,
      "enabled": true
    },
    "filesystem": {
      "transport": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"],
      "description": "File system access"
    }
  }
}
```

**Using MCP Tools in Agents:**
```yaml
tools:
  - mcp:fetch    # All tools from "fetch" server
```

### Agent Orcha Studio

Agent Orcha Studio is a built-in web dashboard available at `http://localhost:3000` when the server is running. It provides a visual interface for managing and testing your entire Agent Orcha setup.

**Studio Tabs:**

| Tab | Description |
|-----|-------------|
| **Agents** | Browse agents, invoke with custom input, stream responses, manage conversation sessions |
| **Knowledge** | Browse and search knowledge stores (vector + GraphRAG), view entities/communities for GraphRAG stores |
| **MCP** | Browse MCP servers, view available tools, call tools directly |
| **Workflows** | Browse and execute workflows (step-based and LangGraph), stream execution progress |
| **IDE** | In-browser file editor with file tree, syntax highlighting (YAML, JSON, JS), save with hot-reload |

## REST API Reference

### Health Check

```
GET /health

Response:
{
  "status": "ok",
  "timestamp": "2026-01-21T12:00:00.000Z"
}
```

### Agents Endpoints

**List All Agents**
```
GET /api/agents

Response:
[
  {
    "name": "agent1",
    "description": "Agent description",
    "version": "1.0.0",
    "tools": ["mcp:fetch", "knowledge:docs"]
  }
]
```

**Get Agent Details**
```
GET /api/agents/:name

Response:
{
  "name": "agent1",
  "description": "Agent description",
  "version": "1.0.0",
  "llm": { "name": "default", "temperature": 0.7 },
  "prompt": { "system": "...", "inputVariables": ["query"] },
  "tools": ["mcp:fetch"],
  "output": { "format": "text" }
}
```

**Invoke Agent**
```
POST /api/agents/:name/invoke
Content-Type: application/json

Request:
{
  "input": {
    "topic": "your topic",
    "context": "additional context"
  },
  "sessionId": "optional-session-id"
}

Response:
{
  "output": "Agent response text",
  "metadata": {
    "tokensUsed": 150,
    "toolCalls": ["fetch", "knowledge_search"],
    "duration": 1234,
    "sessionId": "optional-session-id",
    "messagesInSession": 2
  }
}
```

**Stream Agent Response (SSE)**
```
POST /api/agents/:name/stream
Content-Type: application/json

Request:
{
  "input": {
    "query": "your query"
  },
  "sessionId": "optional-session-id"
}

Response: Server-Sent Events stream with incremental output
```

**Session Statistics**
```
GET /api/agents/sessions/stats

Response:
{
  "activeSessions": 5,
  "totalMessages": 42
}
```

**Get Session Details**
```
GET /api/agents/sessions/:sessionId

Response:
{
  "sessionId": "user-123",
  "messages": [...],
  "messageCount": 10,
  "createdAt": 1706000000000,
  "lastAccessedAt": 1706001000000
}
```

**Delete Session**
```
DELETE /api/agents/sessions/:sessionId

Response:
{
  "success": true
}
```

### Workflows Endpoints

**List All Workflows**
```
GET /api/workflows

Response:
[
  {
    "name": "workflow1",
    "description": "Workflow description",
    "version": "1.0.0",
    "type": "steps"
  }
]
```

**Get Workflow Details**
```
GET /api/workflows/:name

Response:
{
  "name": "workflow1",
  "description": "Workflow description",
  "version": "1.0.0",
  "type": "steps",
  "input": { "schema": {...} },
  "steps": [...],
  "config": { "timeout": 300000 },
  "output": {...}
}
```

**Run Workflow**
```
POST /api/workflows/:name/run
Content-Type: application/json

Request:
{
  "input": {
    "topic": "research topic",
    "style": "professional"
  }
}

Response:
{
  "output": {
    "paper": "Final content",
    "summary": "Key points"
  },
  "metadata": {
    "duration": 5000,
    "stepsExecuted": 3,
    "success": true
  },
  "stepResults": {
    "research": { "output": "...", "metadata": {...} },
    "summarize": { "output": "...", "metadata": {...} }
  }
}
```

**Stream Workflow (SSE)**
```
POST /api/workflows/:name/stream
Content-Type: application/json

Request:
{
  "input": {
    "topic": "research topic"
  }
}

Response: Server-Sent Events stream with step progress updates
```

### Knowledge Endpoints

**List All Knowledge Stores**
```
GET /api/knowledge

Response:
[
  {
    "name": "docs",
    "description": "Knowledge base",
    "kind": "vector",
    "documentCount": 42,
    "storeType": "memory"
  }
]
```

**Get Knowledge Store Config**
```
GET /api/knowledge/:name

Response:
{
  "name": "docs",
  "description": "Knowledge base",
  "kind": "vector",
  "source": {...},
  "loader": {...},
  "splitter": {...},
  "store": {...},
  "search": {...}
}
```

**Search Knowledge Store**
```
POST /api/knowledge/:name/search
Content-Type: application/json

Request:
{
  "query": "search term",
  "k": 4
}

Response:
{
  "results": [
    {
      "content": "Document text...",
      "metadata": {...},
      "score": 0.92
    }
  ]
}
```

**Refresh Knowledge Store (Reload Documents)**
```
POST /api/knowledge/:name/refresh

Response:
{
  "success": true,
  "documentsLoaded": 42,
  "duration": 1234
}
```

**Add Documents to Knowledge Store**
```
POST /api/knowledge/:name/add
Content-Type: application/json

Request:
{
  "documents": [
    {
      "content": "Document text",
      "metadata": { "source": "custom" }
    }
  ]
}

Response:
{
  "success": true,
  "documentsAdded": 1
}
```

**Get GraphRAG Entities**
```
GET /api/knowledge/:name/entities

Response:
[
  {
    "id": "entity-1",
    "type": "Person",
    "name": "Alice",
    "description": "...",
    "properties": {...}
  }
]
```

**Get GraphRAG Communities**
```
GET /api/knowledge/:name/communities

Response:
[
  {
    "id": "community-1",
    "nodeIds": ["entity-1", "entity-2"],
    "summary": "...",
    "title": "..."
  }
]
```

**Get GraphRAG Edges**
```
GET /api/knowledge/:name/edges

Response:
[
  {
    "id": "edge-1",
    "type": "WORKS_WITH",
    "sourceId": "entity-1",
    "targetId": "entity-2",
    "description": "...",
    "weight": 1.0
  }
]
```

### LLM Endpoints

**List All LLM Configurations**
```
GET /api/llm

Response:
[
  {
    "name": "default",
    "model": "gpt-4o",
    "provider": "openai"
  }
]
```

**Get LLM Config**
```
GET /api/llm/:name
```

**Chat with LLM**
```
POST /api/llm/:name/chat
Content-Type: application/json

Request:
{
  "message": "Hello, how are you?"
}

Response:
{
  "output": "I'm doing well, thank you!"
}
```

**Stream LLM Chat (SSE)**
```
POST /api/llm/:name/stream
Content-Type: application/json

Request:
{
  "message": "Hello"
}

Response: Server-Sent Events stream
```

### Functions Endpoints

**List All Functions**
```
GET /api/functions

Response:
[
  {
    "name": "calculator",
    "description": "Performs arithmetic operations",
    "parameters": {...}
  }
]
```

**Get Function Details**
```
GET /api/functions/:name
```

**Call Function**
```
POST /api/functions/:name/call
Content-Type: application/json

Request:
{
  "arguments": {
    "a": 10,
    "b": 5,
    "operation": "add"
  }
}

Response:
{
  "result": "10 + 5 = 15"
}
```

### MCP Endpoints

**List MCP Servers**
```
GET /api/mcp

Response:
[
  {
    "name": "fetch",
    "description": "Web fetch capabilities",
    "transport": "streamable-http",
    "enabled": true
  }
]
```

**Get MCP Server Details**
```
GET /api/mcp/:name
```

**Get MCP Server Tools**
```
GET /api/mcp/:name/tools

Response:
[
  {
    "name": "fetch",
    "description": "Fetch a URL",
    "inputSchema": {...}
  }
]
```

**Call MCP Tool**
```
POST /api/mcp/:name/call
Content-Type: application/json

Request:
{
  "tool": "fetch",
  "arguments": {
    "url": "https://example.com"
  }
}

Response:
{
  "result": "..."
}
```

### Files Endpoints (IDE)

**Get File Tree**
```
GET /api/files/tree

Response:
{
  "tree": [
    {
      "name": "agents",
      "type": "directory",
      "children": [
        { "name": "example.agent.yaml", "type": "file" }
      ]
    }
  ]
}
```

**Read File**
```
GET /api/files/read?path=agents/example.agent.yaml

Response:
{
  "content": "name: example\n..."
}
```

**Write File**
```
PUT /api/files/write
Content-Type: application/json

Request:
{
  "path": "agents/example.agent.yaml",
  "content": "name: example\n..."
}

Response:
{
  "success": true
}
```

## CLI Reference

### Commands

**Initialize Project**
```bash
npx agent-orcha init [directory]

# Creates project structure with examples
# directory - Target directory (default: current)
```

**Start Server**
```bash
npx agent-orcha start

# Starts Fastify server on port 3000 (configurable via PORT env var)
# Agent Orcha Studio available at the same URL
# Environment variables:
#   PORT - Server port (default: 3000)
#   HOST - Server host (default: 0.0.0.0)
#   ORCHA_BASE_DIR - Base directory for config files (default: current directory)
```

**Help**
```bash
npx agent-orcha help

# Shows CLI usage information
```

## Library Usage

Agent Orcha can be used as a TypeScript/JavaScript library in your projects.

```typescript
import { Orchestrator } from 'agent-orcha';

const orchestrator = new Orchestrator({
  projectRoot: './my-agents-project'
});

await orchestrator.initialize();

// Invoke an agent
const result = await orchestrator.agents.invoke('researcher', {
  topic: 'machine learning',
  context: 'brief overview'
});

console.log(result.output);

// Invoke an agent with session memory
const chatResult = await orchestrator.agents.invoke(
  'chatbot',
  { message: 'Hello' },
  { sessionId: 'user-123' }
);

// Run a workflow
const workflowResult = await orchestrator.workflows.run('research-paper', {
  topic: 'artificial intelligence'
});

console.log(workflowResult.output);

// Search knowledge store
const searchResults = await orchestrator.knowledge.search('docs', {
  query: 'semantic search',
  k: 4
});

console.log(searchResults);

// Clean up
await orchestrator.close();
```

## Project Structure

```
my-project/
├── agents/                     # Agent YAML configurations
│   ├── researcher.agent.yaml
│   └── chatbot.agent.yaml
├── workflows/                  # Workflow YAML definitions
│   ├── research.workflow.yaml
│   └── langgraph.workflow.yaml
├── functions/                  # Custom JavaScript functions
│   └── calculator.function.js
├── knowledge/                  # Knowledge store configs and data
│   ├── docs.knowledge.yaml
│   ├── graph.knowledge.yaml
│   └── sample-data/
│       └── documents.txt
├── llm.json                    # LLM and embedding configurations
├── mcp.json                    # MCP server configuration
├── .env                        # Environment variables (optional)
└── package.json                # Project metadata (if using as library)
```

## Environment Variables

```bash
# Server Configuration
PORT=3000                       # Server port (default: 3000)
HOST=0.0.0.0                   # Server host (default: 0.0.0.0)
ORCHA_BASE_DIR=./my-project    # Base directory for config files (default: current directory)
CORS_ORIGIN=true               # CORS origin policy (default: true)

# Note: LLM API keys are configured in llm.json, not via environment variables

# AWS/S3 (for S3 knowledge sources when not in YAML)
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
S3_ENDPOINT=http://localhost:9000  # Custom S3 endpoint (MinIO, Wasabi)

# Database (for database knowledge sources when not in YAML)
DATABASE_URL=postgresql://user:pass@localhost:5432/db
MYSQL_URL=mysql://user:pass@localhost:3306/db
```

## Tool Types Reference

1. **MCP Tools** - External tools from MCP servers
   ```yaml
   tools:
     - mcp:fetch              # All tools from "fetch" server
     - mcp:filesystem         # All tools from "filesystem" server
   ```

2. **Knowledge Tools** - Semantic search on knowledge stores
   ```yaml
   tools:
     - knowledge:docs         # Search "docs" knowledge store
   ```

3. **Function Tools** - Custom JavaScript/TypeScript functions in functions/
   ```yaml
   tools:
     - function:calculator
     - function:fibonacci
   ```

4. **Built-in Tools** - Framework-provided tools
   ```yaml
   tools:
     - builtin:ask_user       # Human-in-the-loop (LangGraph only)
   ```

## Production Deployment

### Docker Deployment

```dockerfile
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
EXPOSE 3000
CMD ["npx", "agent-orcha", "start"]
```

### Knowledge Store Setup

**Chroma (Persistent Vector Store):**
```bash
# Run Chroma with Docker
docker run -p 8000:8000 chromadb/chroma

# Or install and run locally
pip install chromadb
chroma run --path .chroma --port 8000
```

**Neo4j (Persistent Graph Store for GraphRAG):**
```bash
# Run Neo4j with Docker
docker run -p 7474:7474 -p 7687:7687 neo4j
```

### Configuration Best Practices

1. **LLM Configuration**: Store API keys in environment variables, not in llm.json
2. **Knowledge Stores**: Use Chroma/Pinecone/Qdrant for production (persistent), Memory for development
3. **Timeouts**: Configure appropriate timeouts for workflows based on expected duration
4. **Error Handling**: Set onError strategy in workflows (stop, continue, retry)
5. **GraphRAG Caching**: Enable graph caching to avoid re-extracting entities on restart

## Examples

### Example 1: Simple Q&A Agent

```yaml
# agents/qa.agent.yaml
name: qa
description: Question answering agent
version: "1.0.0"

llm:
  name: default
  temperature: 0.3

prompt:
  system: |
    You are a helpful Q&A assistant.
    Provide concise, accurate answers to questions.
  inputVariables:
    - question

output:
  format: text
```

### Example 2: Research Agent with Knowledge Search

```yaml
# agents/researcher.agent.yaml
name: researcher
description: Research agent with web fetch and knowledge search
version: "1.0.0"

llm:
  name: default
  temperature: 0.5

prompt:
  system: |
    You are a thorough researcher.
    1. Search the knowledge base first
    2. Fetch additional information from the web if needed
    3. Synthesize findings into a comprehensive report
  inputVariables:
    - topic
    - context

tools:
  - mcp:fetch
  - knowledge:docs

output:
  format: text
```

### Example 3: Multi-Step Workflow

```yaml
# workflows/analysis.workflow.yaml
name: analysis
description: Multi-step data analysis workflow
version: "1.0.0"

input:
  schema:
    data:
      type: string
      required: true

steps:
  - id: extract
    agent: extractor
    input:
      data: "{{input.data}}"
    output:
      key: extracted

  - id: analyze
    agent: analyzer
    input:
      data: "{{steps.extract.output}}"
    output:
      key: analysis

  - id: summarize
    agent: summarizer
    input:
      analysis: "{{steps.analyze.output}}"
    output:
      key: summary

config:
  timeout: 300000
  onError: stop

output:
  summary: "{{steps.summarize.output}}"
  fullAnalysis: "{{steps.analyze.output}}"
```

### Example 4: LangGraph Autonomous Workflow

```yaml
# workflows/autonomous.workflow.yaml
name: autonomous-research
description: Autonomous research using LangGraph
version: "1.0.0"
type: langgraph

input:
  schema:
    topic:
      type: string
      required: true

prompt:
  system: |
    You are a research assistant with access to tools and agents.
    Use available tools to gather information, then synthesize a comprehensive report.
  goal: "Research and provide a thorough analysis of: {{input.topic}}"

graph:
  model: default
  executionMode: react
  tools:
    mode: all
    sources: [mcp, knowledge, function, builtin]
  agents:
    mode: all
  maxIterations: 10
  timeout: 300000

output:
  analysis: "{{state.messages[-1].content}}"
```

### Example 5: Agent with Structured Output

```yaml
# agents/data-extractor.agent.yaml
name: data-extractor
description: Extracts structured data from text
version: "1.0.0"

llm:
  name: default
  temperature: 0

prompt:
  system: |
    Extract structured information from the provided text.
  inputVariables:
    - text

output:
  format: structured
  schema:
    type: object
    properties:
      people:
        type: array
        items:
          type: object
      locations:
        type: array
        items:
          type: string
      dates:
        type: array
        items:
          type: string
    required:
      - people
      - locations
```

### Example 6: GraphRAG Knowledge Store

```yaml
# knowledge/transcripts.knowledge.yaml
name: transcripts
kind: graph-rag
description: Transcript analysis with entity extraction

source:
  type: directory
  path: knowledge/transcripts
  pattern: "*.txt"

loader:
  type: text

splitter:
  type: recursive
  chunkSize: 2000
  chunkOverlap: 200

embedding: default

graph:
  extraction:
    llm: default
    entityTypes:
      - name: Person
        description: "A person mentioned in the text"
      - name: Organization
        description: "A company or organization"
      - name: Topic
        description: "A subject being discussed"
    relationshipTypes:
      - name: WORKS_FOR
        description: "Person works for organization"
      - name: DISCUSSED
        description: "Topic was discussed"
  communities:
    algorithm: louvain
    resolution: 1.0
    minSize: 2
    summaryLlm: default
  store:
    type: memory
  cache:
    enabled: true
    directory: .graph-cache

search:
  defaultK: 10
  localSearch:
    maxDepth: 2
  globalSearch:
    topCommunities: 5
    llm: default
```

## Troubleshooting

### Common Issues

1. **LLM Connection Errors**
   - Verify API keys in llm.json or environment variables
   - Check baseUrl for local models (LM Studio, Ollama)
   - Ensure model name matches provider configuration

2. **Knowledge Store Errors**
   - For Chroma: Verify Chroma server is running on specified URL
   - Check embedding model configuration in llm.json
   - Ensure source documents exist at specified paths
   - For GraphRAG: Check that extraction LLM is configured and accessible

3. **MCP Server Errors**
   - Verify MCP server is accessible at specified URL
   - Check timeout configuration
   - Review MCP server logs for errors

4. **Workflow Execution Errors**
   - Check step dependencies and template syntax
   - Verify agent names referenced in steps exist
   - Review timeout configuration for long-running workflows
   - For LangGraph: Check maxIterations to prevent runaway loops

5. **Session/Memory Issues**
   - Ensure consistent sessionId across requests
   - Check session TTL hasn't expired
   - Memory is in-process; sessions are lost on server restart

## Additional Resources

- **Documentation:** https://ddalcu.github.io/agent-orcha/documentation.html
- **GitHub Repository:** https://github.com/ddalcu/agent-orcha
- **NPM Package:** https://www.npmjs.com/package/agent-orcha
- **Issue Tracker:** https://github.com/ddalcu/agent-orcha/issues
- **Discussions:** https://github.com/ddalcu/agent-orcha/discussions

## Version History

- **0.0.3** (Current)
  - Knowledge stores replace vector stores (vectors/ -> knowledge/, .vector.yaml -> .knowledge.yaml)
  - GraphRAG knowledge stores with entity extraction and community detection
  - LangGraph autonomous workflows with tool/agent discovery
  - Agent Orcha Studio web dashboard with in-browser IDE
  - Conversation memory with session management
  - Structured output with JSON schema validation
  - New data sources: database (PostgreSQL/MySQL), S3 (AWS/MinIO), web scraping
  - New store types: Pinecone, Qdrant
  - Custom functions API and MCP management API
  - Files API for in-browser editing with hot-reload

- **0.0.2**
  - Streaming improvements
  - Bug fixes and stability

- **0.0.1** (Initial Release)
  - Declarative YAML-based agent configuration
  - Multi-agent workflow orchestration
  - Vector store integration (Memory, Chroma)
  - MCP server support
  - Custom function tools
  - REST API with SSE streaming
  - CLI for project initialization and server management
  - Web UI for agent and workflow execution

---

Last Updated: 2026-01-29
